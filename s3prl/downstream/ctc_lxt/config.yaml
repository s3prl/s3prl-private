runner:
  total_steps: 1000
  gradient_clipping: 1
  gradient_accumulate_steps: 2

  log_step: 100
  eval_step: 200
  save_step: 100
  max_keep: 1
  eval_dataloaders:
    - dev

optimizer:
  name: TorchOptim
  torch_optim_name: Adam
  lr: 1.0e-2

# comment the whole scheduler config block
# to disable learning rate scheduling
# scheduler:
#   name: linear_schedule_with_warmup
#   num_warmup_steps: 1400

# comment the whole specaug config block
# to disable specaug on representation
# specaug:
#   apply_time_warp: true
#   apply_time_mask: true
#   apply_freq_mask: true
#   time_warp_window: 3
#   time_mask_width_range: [0, 10]
#   freq_mask_width_range: [0, 10]
#   num_freq_mask: 10
#   num_time_mask: 10

downstream_expert:
  corpus:
    name: 'lxtphone'                   # Specify corpus
    path: '/path/to/hidden/set/1.1_distributed'          # Path to raw LibriSpeech dataset
    lexicon:
      - downstream/ctc_lxt/lexicon/lxt_lexicon_g2p.txt

    train: ['downstream/ctc_lxt/data/train.txt']                # Name of data splits to be used as training set
    dev: ['downstream/ctc_lxt/data/dev.txt']                    # Name of data splits to be used as validation set
    test: ['downstream/ctc_lxt/data/test.txt']

    transcriptions:
      train: ['downstream/ctc_lxt/data/train_transcriptions.txt']                # Name of data splits to be used as training set
      dev: ['downstream/ctc_lxt/data/dev_transcriptions.txt']                    # Name of data splits to be used as validation set
      test: ['downstream/ctc_lxt/data/test_transcriptions.txt']

    bucketing: False                       # Enable/Disable bucketing 
    batch_size: 16
    num_workers: 1

  text:
    mode: 'word'                       # 'character'/'word'/'subword'
    vocab_file: 'downstream/ctc_lxt/vocab/phoneme.txt'

  model:
    project_dim: 256
    zero_infinity: True

    select: FrameLevel
    Wav2Letter:
      total_rate: 320
    RNNs:
      total_rate: 320
      module: 'LSTM'                        # 'LSTM'/'GRU'
      bidirection: True
      dim: [1024, 1024, 1024]
      dropout: [0.2, 0.2, 0.2]
      layer_norm: [True, True, True]
      proj: [True, True, True]              # Linear projection + Tanh after each rnn layer
      sample_rate: [1, 1, 1]
      sample_style: 'concat'                  # 'drop'/'concat'

  save_best_on:
    - dev

  metric_higher_better: False
  metric:  # The first metric will be used to save checkpoint
    - wer

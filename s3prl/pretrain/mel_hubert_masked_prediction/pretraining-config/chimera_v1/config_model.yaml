distiller:
  # Extractor
  extractor_mode: mel # if 'mel', no convolution feature extractor, and 'mel_dim' is 80.
  extractor_conv_feature_layers: "[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2"
  extractor_dropout: 0.0
  feature_grad_mult: 0.1

  # Convolutional relative positional encoding
  conv_pos: 128
  conv_pos_groups: 16

  # Transformer encoder
  encoder_layers: 2
  encoder_embed_dim: 768
  encoder_ffn_embed_dim: 3072
  encoder_attention_heads: 12
  activation_fn: gelu
  layer_norm_first: false
  attention_type: original

  # Dropout
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.1
  encoder_layerdrop: 0.0

  # Output
  final_dim: 768
  out_layer_type: expand-last

  # Task & loss
  n_tasks: 3
  task_emb_type: expand-last
  loss_type: l1
  feat_pen_loss: 0.0
  cosine_loss: 1.0 # cosine similarity loss
  pred_layer_id: [4, 8, 12]

  # [Project Chimera] Initialization
  # set init_teacher_conv_layers to false since student has no conv. layers
  # initialize student's encoder with melhubert's encoder
  init_teacher_conv_layers: false
  init_teacher_encoder_layers: false

  init_melhubert_encoder_layers: true
  melhubert_ckpt: /home/ga642381/chimera2022/chimera-s3prl/pretrained_models/short-melhubert-stage2-libri960-50epochs.ckpt

teacher:
  model: hubert_base_robust_mgwham_rbp
  n_layers: 12

task:
  sequence_length: 250000 # 15.6 secs

audio:
  target_level: None
